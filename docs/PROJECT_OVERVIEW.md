# PLURA（プルーラ） — プロジェクト概要

> **「自分だけのノートから、みんなの集合知へ」**

---

## 0. ビジョンとミッション

### dx-junkyard ビジョン・ステートメント

> 「小さな熱源が有機的につながり、変革が連鎖する社会へ。
> 私たちが目指すのは、技術や知見の共有を通じて、小さなコミュニティが草の根でつながり合う包摂的な社会です。属性や場所を問わず、誰もが創造性を発揮できる環境を整えることで、社会の課題を希望へと変えていく未来を実現します。」

### dx-junkyard ミッション・ステートメント

> 「『ジャンク』を価値に、失敗を共有知に。
> dx-junkyardは、過去の試行錯誤から再利用可能なリソースを掘り起こし、ITの力で新たな価値へと昇華させます。私たちは、個人のスキルと経験を惜しみなく共有し、誰もが自立して課題に挑める『創造的共助』のコミュニティを育成します。」

### PLURAの位置づけ

AI Agentの進化は、個人の能力を劇的に拡張する。これまでチームでなければ成し遂げられなかったことが、AIを相棒にすることで一人でも完遂できる時代が来ている。

個人のカバー範囲が広がるにつれ、固定的な部署や階層構造は意味を失い、組織はより**フラットで流動的なもの（Liquid Organization）**へと姿を変えていく。

その時に必要なのは「管理システム」ではなく、**個人の熱量を損なうことなく、必要な瞬間に必要な才能が自然と結びつく「生態系」**である。PLURAは、この生態系の神経回路として機能するAI Agent Networkである。

---

## 1. 課題の新規性 ── 誰もが抱えているのに、まだ解かれていない問題

### 1-1. 課題の規模：知識労働者の87%が「暗黙知の損失」を経験している

知識労働者（エンジニア、デザイナー、企画職、マネージャー等）は、日々膨大な量の「思考」を行っている。しかし、その大部分は記録されず消えていく。

- 会議後の「なんか引っかかった」 → メモせずに忘れる
- 試行錯誤で得た知見 → 成果物にしか残らず、プロセスが消える
- 退職・異動時のノウハウ → 引き継ぎ書では伝わらない

McKinsey の調査によれば、知識労働者は業務時間の**19.8%を情報検索**に費やしている。これは「情報がない」のではなく、**組織のどこかに既にある知見にアクセスできない**ことが原因である。

### 1-2. なぜ今まで解かれなかったのか？ ── 3つの構造的障壁

既存ツールがこの問題を解けないのは、単なる機能不足ではなく、**根本的な設計思想の矛盾**に原因がある。

```
         既存ツールのジレンマ
         ━━━━━━━━━━━━━━━━

  「記録」と「共有」を同時に要求してしまう
                 ↓
  ┌─────────────────────────────────────────┐
  │                                         │
  │  障壁①：言語化コスト                     │
  │  ─────────────────                      │
  │  共有するには「きれいな文章」が必要。      │
  │  しかし思考はそもそも断片的で混沌として    │
  │  いる。整理してから書く負担が大きすぎて、  │
  │  結局誰も書かない。                       │
  │                                         │
  │  障壁②：プライバシーの壁                  │
  │  ─────────────────                      │
  │  本音や迷いを記録したい。しかし共有前提の  │
  │  ツールでは、見られることを意識して        │
  │  「建前」しか書けない。最も価値のある      │
  │  「泥臭い試行錯誤」が記録されない。        │
  │                                         │
  │  障壁③：共有の動機設計                    │
  │  ─────────────────                      │
  │  「ドキュメントを書いて共有してください」   │
  │  は追加の作業負荷。個人にとっての直接的な  │
  │  メリットがないため、ドキュメント文化は     │
  │  定着しない。                              │
  │                                         │
  └─────────────────────────────────────────┘
```

**PLURAの発見：3つの障壁は「同じ場所」で記録と共有をさせようとするから生じる。**

記録する場所（Private）と共有する場所（Public）を**構造的に分離し、AIが間を橋渡し**すれば、すべての障壁が同時に解消される。これがPLURAの3層アーキテクチャの着想である。

### 1-3. 既存ソリューションとの根本的な違い

| ツール | 記録 | 共有 | 根本的限界 |
|--------|:----:|:----:|------|
| メモアプリ（Notion, Obsidian） | ○ | △ | 「整理された文章」を書く前提。共有は手動の追加作業 |
| チャット（Slack, Teams） | △ | ○ | 情報が流れる。深い思考には不向き。全てが見られる前提 |
| ドキュメント（Confluence） | △ | ○ | 書くハードルが高い。更新されず陳腐化 |
| SNS・マッチング | × | △ | 「正解」しか流通しない。試行錯誤や失敗は共有されない |
| AI チャット（ChatGPT） | ○ | × | **個人完結**。組織の文脈がない。知見が他者に還元されない |
| **PLURA** | **○** | **○** | **記録と共有を分離し、AIが自動で橋渡し。追加作業ゼロ** |

既存のAIチャットは「優秀な個人アシスタント」だが、生成された知見は個人の中に閉じたまま消える。PLURAは**個人の思考プロセスを記録しながら、その副産物として組織の集合知を自動生成する** ── この「記録」と「共有」の構造的分離こそが、本プロジェクト固有の課題発見である。

---

## 2. 解決策の有効性 ── 課題にどう効くのか

### 2-1. コアコンセプト：3層アーキテクチャ

![PLURA システムアーキテクチャ](./plura_architecture.png)

PLURAの3層構造は、セクション1で述べた**3つの構造的障壁を、それぞれ専用のレイヤーで解消する**設計になっている。

```
  障壁              →    解消するレイヤー
  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  ①言語化コスト      →    Layer 1: Private Safehouse
                          断片的な言葉でもAIが文脈を補完。
                          「整理してから書く」必要がない。

  ②プライバシーの壁  →    Layer 2: Gateway Refinery
                          個人情報を自動除去・抽象化。
                          承認なしには絶対に外に出ない。

  ③共有の動機設計    →    Layer 2 → 3 の自動パイプライン
                          記録するだけで共有が「副作用」として
                          発生。追加作業ゼロ。
```

### 2-2. 機能と課題の対応マップ

各機能が、どの課題をどう解決するかを示す。

| 課題 | 解決する機能 | 仕組み |
|------|-------------|--------|
| **①思考の揮発性** | ThoughtStream + Challenge Readiness | チャット形式で対話するだけで記録が蓄積。AIが文脈を読み、共感・深掘り・問題提起を自動で使い分け、対話を続けたくなる設計 |
| **②言語化の壁** | ThoughtStream + 音声入力 | 「うまく言えなくても大丈夫」。断片的な言葉をAIが補完・構造化。音声入力で歩きながらでも記録可能 |
| **③内省の継続困難** | User Profiling + Deep Research | 過去の思考傾向を可視化し「前はこう考えていた」と気づかせる。気になるテーマはAIが自動で深掘り調査 |
| **④知識のサイロ化** | Insight Pipeline + Knowledge Store | 個人ログからAIが自動で共有可能なインサイトを抽出。ベクトル検索で部署を超えて「似た課題」が結びつく |
| **⑤暗黙知の消失** | Insight Pipeline + Privacy Sanitizer | 個人情報を除去した上で Context/Problem/Solution に構造化。人が去っても知見が残る |
| **⑥セレンディピティの不在** | Serendipity Matcher | 入力中にリアルタイムで関連インサイトを推薦。「検索しに行く」のではなく、**書いているだけで発見が起きる** |

### 2-3. 主要機能の詳細

#### Layer 1: Private Safehouse（思考の私有地）── 障壁①②を解消

**ThoughtStream**
- チャット形式の入力で「対話」として自然に記録。メモを書く心理的ハードルを除去
- Whisper APIによる音声入力。歩きながら、通勤中に記録可能

**Challenge Readiness（適応的応答）**
- AIがユーザーの状態を0.0〜1.0のスコアで推定し、応答トーンを自動調整
  - **gentle（0.0〜0.3）**: 愚痴・感情の吐き出し → 共感に徹する
  - **supportive（0.3〜0.7）**: 思考の整理中 → 寄り添いながら質問で深掘り
  - **challenging（0.7〜1.0）**: 深い議論を求めている → 鋭い指摘や問題提起
- LangGraph による動的ルーティングで、意図に応じて専用ノード（chat / empathy / deep_dive / brainstorm / knowledge）に振り分け

**Deep Research（深掘り調査）**
- ユーザーの問いに対して、調査計画を提案 → 承認 → バックグラウンドで包括的な調査レポートを自動生成
- 個人の「気になる」を本格的なリサーチに昇華し、記録の継続動機を生む

#### Layer 2: Gateway Refinery（情報の関所）── 障壁②③を解消

**Privacy Sanitizer**
- LLMによる個人名・日付・固有名詞の自動マスク・一般化
- ユーザーは何も気にせず書ける。プライバシーはシステムが守る

**Insight Distiller**
- 思考ログを Context（背景）/ Problem（課題）/ Solution（解決策）に自動構造化
- 整理されていない断片から、他者に伝わる形式のインサイトカードを生成

**Sharing Broker**
- 共有価値スコアリング（0-100）で他者にとっての有用性を自動評価
- **スコアが高いものだけ**をユーザーに提案。**明示的な承認がない限り絶対に公開しない**
- 「書くだけで共有が起きる」が、コントロールは常にユーザーの手にある

#### Layer 3: Public Plaza（共創の広場）── 課題④⑤⑥を解消

**Knowledge Store（Qdrant ベクトル検索）**
- 承認されたインサイトをベクトル埋め込みで格納
- キーワード一致ではなく「意味的に似た課題」を発見

**Serendipity Matcher**
- ユーザーが入力している最中に、リアルタイムで関連インサイトを推薦
- 「あなたと同じ課題を、別の人がこう解決しました」── 偶然の発見をデジタルで再現

**Just-in-Time Teaming（将来構想）**
- AIが思考ログの文脈を解析し、必要なスキルと熱量を持ったメンバーを自動で結びつける
- マッチングを目的にしない。自分のための記録の「副作用」として、最適な他者と接続される

### 2-4. 有効性の検証ポイント

| 指標 | 測定方法 | 有効性の判断基準 |
|------|----------|-----------------|
| 記録継続率 | 7日間連続投稿率 | メモアプリの平均（約15%）を上回る |
| 言語化コスト低減 | 初回投稿までの所要時間 | 30秒以内に最初のメッセージを送信 |
| インサイト生成率 | 投稿→インサイトカード化の割合 | 投稿の20%以上が共有可能な品質に達する |
| セレンディピティ発生率 | マッチング提示→ユーザー閲覧率 | 提示の30%以上が閲覧される |
| 知識再利用率 | Thanks数 / インサイト閲覧数 | 閲覧の5%以上がThankを送信 |

---

## 3. 実装品質と拡張性 ── どう作り、どう育てるか

### 3-1. 現在の実装状況

| レイヤー | コンポーネント | 実装状態 | 備考 |
|----------|--------------|:--------:|------|
| **Layer 1** | ThoughtStream（チャットUI） | **稼働中** | テキスト + 音声入力対応 |
| | Context Analyzer（感情・意図解析） | **稼働中** | マルチLLM対応 |
| | LangGraph Router（動的ルーティング） | **稼働中** | 7ノード実装済 |
| | Challenge Readiness（適応的応答） | **稼働中** | ルールベース + LLM推定のハイブリッド |
| | Structural Analyzer（構造分析） | **稼働中** | 非同期Celeryタスク |
| | Deep Research（深掘り調査） | **稼働中** | 計画提案→承認→非同期レポート生成 |
| | User Profiler（思考傾向） | **稼働中** | インクリメンタル更新 |
| **Layer 2** | Privacy Sanitizer | **稼働中** | LLMベースPII除去 |
| | Insight Distiller | **稼働中** | Context/Problem/Solution構造化 |
| | Sharing Broker | **稼働中** | スコアリング + 承認フロー |
| **Layer 3** | Knowledge Store | **稼働中** | Qdrant ベクトル検索 |
| | Serendipity Matcher | **稼働中** | リアルタイム推薦 |
| | Just-in-Time Teaming | 構想段階 | Phase 4で実装予定 |

### 3-2. アーキテクチャの設計思想

#### (a) レイヤー分離による関心の分離

3層アーキテクチャは単なる概念モデルではなく、**コードベース上で物理的にディレクトリ分離**されている。

```
backend/app/services/
├── layer1/     # Private: 思考キャプチャ・AI対話
│   ├── conversation_graph.py    # LangGraph 状態機械
│   ├── nodes/                   # 意図別の専用ノード群
│   │   ├── chat_node.py
│   │   ├── empathy_node.py
│   │   ├── deep_dive_node.py
│   │   ├── brainstorm_node.py
│   │   └── knowledge_node.py
│   ├── intent_router.py         # 仮説駆動型意図分類
│   ├── situation_router.py      # ルールベース状況分類
│   ├── context_analyzer.py      # 感情・トピック解析
│   ├── structural_analyzer.py   # 構造的課題の追跡
│   └── user_profiler.py         # 思考傾向プロファイル
├── layer2/     # Gateway: プライバシー保護・精製
│   ├── privacy_sanitizer.py     # PII除去
│   ├── insight_distiller.py     # 構造化
│   └── sharing_broker.py        # 共有価値評価
└── layer3/     # Public: 検索・マッチング
    ├── knowledge_store.py       # Qdrant ベクトルDB
    └── serendipity_matcher.py   # 推薦エンジン
```

この分離により、**Layer 2を通過せずにデータがLayer 3に流れることは構造上不可能**であり、プライバシー保護がアーキテクチャレベルで強制される（Privacy by Design）。

#### (b) LangGraph による動的ルーティング ── 拡張可能な会話制御

会話制御に LangGraph（状態機械ベース）を採用。新しい会話モードの追加は**ノードを1つ追加してルーターに登録するだけ**で完了する。

```
                    ┌─── chat_node
                    ├─── empathy_node
User Input → Router ├─── deep_dive_node    → Response
                    ├─── brainstorm_node
                    ├─── knowledge_node
                    └─── (新しいノードをここに追加)
```

- **現在**: 7ノード（chat, empathy, deep_dive, brainstorm, knowledge, research_proposal, deep_research）
- **拡張例**: メンタリングノード、振り返りノード、チーム提案ノード等を追加可能
- 各ノードは独立したPythonモジュールで、他ノードへの依存なし

#### (c) マルチLLMアーキテクチャ ── コスト最適化と品質の両立

用途別に3段階のLLMティアを設計。**処理の重要度に応じてモデルを自動選択**し、コストと品質を最適化する。

| ティア | 用途 | モデル例 | 特徴 |
|--------|------|----------|------|
| **FAST** | Context Analyzer, Intent Router | gpt-5-nano | 低レイテンシ（<500ms）。リアルタイム解析 |
| **BALANCED** | Privacy Sanitizer, Insight Distiller, Sharing Broker | gpt-5-mini | バランス重視。Layer 2の標準処理 |
| **DEEP** | Structural Analyzer, Deep Research, 会話ノード | gpt-5.2 | 品質最優先。深い洞察・構造化 |

**プロバイダー切替**: 環境変数1つで OpenAI ↔ Google Vertex AI（Gemini）を切替可能。ベンダーロックインを回避。

```bash
# OpenAI → Gemini への切替（設定変更のみ、コード変更なし）
LLM_CONFIG_DEEP='{"provider": "vertex", "model": "gemini-1.5-pro"}'
```

#### (d) 非同期パイプライン ── UXを損なわない重い処理

Celery + Redis による非同期タスクキューで、**ユーザーの入力→応答のレイテンシを犠牲にせず**、重い処理をバックグラウンドで実行。

```
ユーザー入力
  │
  ├─→ 即時応答（<2秒）: LangGraph → AI返答
  │
  └─→ バックグラウンド（数秒〜数分）:
       ├─ Celery: 構造分析（Structural Analyzer）
       ├─ Celery: インサイト精製パイプライン（Layer 2 全工程）
       ├─ Celery: ユーザープロファイル更新
       └─ Celery: Deep Research レポート生成
```

タスクはレイヤー別キュー（`layer1`, `layer2`, `celery`）に振り分けられ、優先度制御が可能。

### 3-3. 拡張性の設計

| 拡張方向 | 設計上の対応 | 具体例 |
|----------|------------|--------|
| **新しい会話モード** | LangGraphノード追加（1ファイル + ルーター登録） | メンタリング、振り返り、チーム提案 |
| **新しいLLMプロバイダー** | Provider抽象化層（`core/providers/`） | Claude, Mistral, ローカルLLM等の追加 |
| **新しいEmbeddingモデル** | 設定変更のみ（コード変更不要） | OpenAI, Vertex AI, Cohere等 |
| **新しいLayer 2フィルター** | パイプラインにステージ追加 | 著作権チェック、ファクトチェック等 |
| **スケールアウト** | Docker Compose → Kubernetes | Celeryワーカーの水平スケーリング |
| **多言語対応** | プロンプトの差し替え + Embedding切替 | 英語・中国語等 |

### 3-4. 技術スタックと選定理由

| カテゴリ | 技術 | 選定理由 |
|----------|------|----------|
| **フロントエンド** | Next.js 14, React, TypeScript, Tailwind CSS, Zustand | App Routerによるストリーミング対応。型安全性。軽量な状態管理 |
| **バックエンド** | FastAPI, Python 3.11, SQLAlchemy (async), Pydantic | async/awaitネイティブで高スループット。LLMエコシステムとの親和性 |
| **RDB** | PostgreSQL 16 | JSONB対応で柔軟なスキーマ拡張。実績と信頼性 |
| **ベクトルDB** | Qdrant | 高速な近似最近傍探索。フィルタ付き検索対応。OSS |
| **タスクキュー** | Celery + Redis 7 | 実績のある非同期処理基盤。レイヤー別キュー分離 |
| **LLM** | OpenAI / Google Vertex AI | マルチプロバイダーでベンダーロックイン回避。用途別モデル選択 |
| **会話制御** | LangGraph | 状態機械ベースの宣言的フロー。ノード単位の拡張性 |
| **インフラ** | Docker Compose | ワンコマンドで全サービス起動。開発・本番環境の統一 |

### 3-5. 運用コストの最適化

```
コスト最適化の3つの戦略:

1. LLMティア分離
   ─────────────
   全処理にGPT-5を使わない。
   リアルタイム解析は軽量モデル（FAST）、
   品質が必要な処理のみ高性能モデル（DEEP）。
   → コストを最大1/10に圧縮

2. 非同期バッチ処理
   ─────────────
   インサイト精製は即時性不要。
   Celeryキューに積んでバッチ処理。
   → ピーク負荷を平準化

3. ベクトル検索の活用
   ─────────────
   類似インサイト検索にLLMを使わない。
   Qdrantのベクトル検索で高速・低コストに実現。
   → 検索コストを事実上ゼロに
```

---

## 4. ロードマップ

| フェーズ | 焦点 | 主な実装内容 | KPI |
|----------|------|-------------|-----|
| **Phase 1（現在）** | 最高の「独り言ツール」 | ThoughtStream, LangGraph, Challenge Readiness, Deep Research | DAU、記録継続率 |
| **Phase 2** | 「精製所」の稼働 | Insight Pipeline全自動化、承認UXの洗練 | インサイト生成数、承認率 |
| **Phase 3** | 「偶然の結合」の実現 | Serendipity Matcherの精度向上、組織横断検索 | マッチング数、Thanks数 |
| **Phase 4** | Just-in-Time Teaming | Flash Team結成、AI仲介チャットルーム | チーム結成数、プロトタイプ創出数 |

---

## 5. Just-in-Time Teaming ── 将来構想

PLURAの究極のゴールは、**「マッチングを目的にしないマッチング」**である。

個人がAI Agentと対話しながら記録した思考ログの海から、AIがバックグラウンドで文脈を解析し、**その瞬間に必要なスキルと熱量を持ったメンバーを見つけ出し、即座に撚り合わせる**。

```
┌──────────────────────────────────────────────────────────────┐
│  具体例：3つの孤立した「熱源」が結びつく瞬間                     │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  マーケターA  ──  「Z世代の所有欲を深掘り中。                   │
│                    データはあるが技術が見えない…」              │
│                                                              │
│  エンジニアB  ──  「ブロックチェーンで時間単位の所有権移転       │
│                    コードを書いた。面白いが使い道がない…」      │
│                                                              │
│  デザイナーC  ──  「デジタルで"手触り"を感じるUIを作りたい。    │
│                    この表現が生きる企画はないか…」              │
│                                                              │
│  ─────────────────────────────────────────                    │
│  AIが発見：                                                    │
│  ・Aの「課題」は Bの「ジャンク（コード）」で解決できる          │
│  ・Bの「技術」は Cの「UI」があって初めてユーザーに届く          │
│                                                              │
│  → 3つの「ジャンク」が組み合わさり、                           │
│    革新的なプロトタイプが誕生                                   │
└──────────────────────────────────────────────────────────────┘
```

これは組織を「堅牢な城」ではなく、**「織物（タペストリー）」**として捉える発想である。壮大なビジョンという大きな絵を織り上げるために、その時必要な**一本の糸（Flash Team）**を即席で集める。課題が解決されれば糸は織物の一部として固定され（知見の定着）、チームは解散してまた次の糸へと戻る。

---

## 参考

- [ビジョンを織り、糸（チーム）を紡ぐ、PLURA](https://zenn.dev/uras/articles/b3cf8578668440) — dx-junkyard / uras
