"""
MINDYARD - Knowledge Node (å¯¾è©±å‹ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ»ãƒªã‚µãƒ¼ãƒå¯¾å¿œ)
çŸ¥è­˜è¦æ±‚ã«å¯¾ã—ã¦å†…éƒ¨çŸ¥è­˜ï¼ˆLayer 3ï¼‰ã‚’æ¤œç´¢ã—å›ç­”ã‚’ç”Ÿæˆã€‚
å†…éƒ¨æƒ…å ±ã ã‘ã§ã¯ä¸ååˆ†ãªå ´åˆã€è¿½åŠ èª¿æŸ»ã®ææ¡ˆã‚’è¡Œã†ã€‚

UXãƒã‚¤ãƒ³ãƒˆ:
- åŒæœŸå‡¦ç†: ã¾ãšå†…éƒ¨DBï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼‰ã®çµæœã§å³æ™‚å›ç­”ã‚’ç”Ÿæˆ
- èª¿æŸ»ææ¡ˆ: å†…éƒ¨æƒ…å ±ã ã‘ã§ã¯ä¸ååˆ†ãªå ´åˆã€å¤–éƒ¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ»ãƒªã‚µãƒ¼ãƒã‚’ææ¡ˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æ‰¿èª: ææ¡ˆã‚’å—è«¾ã—ãŸå ´åˆã®ã¿ã€research_trigger_node ã§éåŒæœŸèª¿æŸ»ã‚’ç™ºç«
"""
from typing import Any, Dict, List, Optional

from app.core.llm import llm_manager
from app.core.llm_provider import LLMProvider, LLMUsageRole
from app.core.logger import get_traced_logger

logger = get_traced_logger("KnowledgeNode")

_SYSTEM_PROMPT = """ã‚ãªãŸã¯MINDYARDã®ãƒŠãƒ¬ãƒƒã‚¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŸ¥è­˜è¦æ±‚ã«å¯¾ã—ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ãƒ«ãƒ¼ãƒ«:
- ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã™ã‚‹
- ç¢ºä¿¡ãŒãªã„å ´åˆã¯ã€Œã€œã¨è€ƒãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€ç­‰ã®è¡¨ç¾ã‚’ä½¿ã†
- å°‚é–€ç”¨èªã¯å™›ã¿ç •ã„ã¦èª¬æ˜ã™ã‚‹
- æ—¥æœ¬èªã§å¿œç­”ã™ã‚‹
"""

_KNOWLEDGE_ANSWER_PROMPT = """ã‚ãªãŸã¯MINDYARDã®ãƒŠãƒ¬ãƒƒã‚¸ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®å†…éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢çµæœã‚’å‚è€ƒã«ã—ã¤ã¤å›ç­”ã—ã¦ãã ã•ã„ã€‚

## å†…éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æ¤œç´¢çµæœ:
{knowledge_context}

## ãƒ«ãƒ¼ãƒ«:
- æ¤œç´¢çµæœã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ç©æ¥µçš„ã«å¼•ç”¨ã™ã‚‹
- æ¤œç´¢çµæœãŒãªã„å ´åˆã‚„é–¢é€£æ€§ãŒä½ã„å ´åˆã¯ã€ä¸€èˆ¬çŸ¥è­˜ã§å›ç­”ã™ã‚‹
- ç¢ºä¿¡ãŒãªã„å ´åˆã¯ã€Œã€œã¨è€ƒãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€ç­‰ã®è¡¨ç¾ã‚’ä½¿ã†
- æ—¥æœ¬èªã§å¿œç­”ã™ã‚‹
"""

_SUFFICIENCY_CHECK_PROMPT = """ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨ã€å†…éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢çµæœã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query}

å†…éƒ¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢çµæœ:
{knowledge_context}

ç”Ÿæˆã—ãŸå›ç­”:
{answer}

ä»¥ä¸‹ã®è¦³ç‚¹ã§åˆ¤å®šã—ã¦ãã ã•ã„:
- å†…éƒ¨çŸ¥è­˜ã ã‘ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ååˆ†ã«å›ç­”ã§ãã¦ã„ã‚‹ã‹
- æœ€æ–°ã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚„å°‚é–€çš„ãªæ–‡çŒ®ãŒå¿…è¦ã‹
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•ã„ãŒåºƒç¯„ã§ã€è¿½åŠ èª¿æŸ»ãŒæœ‰ç›Šã‹

å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„:
{{
    "is_sufficient": true | false,
    "reason": "åˆ¤å®šç†ç”±",
    "suggested_research_query": "å¤–éƒ¨èª¿æŸ»ãŒå¿…è¦ãªå ´åˆã®æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆä¸è¦ãªã‚‰nullï¼‰"
}}"""


def _get_provider() -> Optional[LLMProvider]:
    try:
        return llm_manager.get_client(LLMUsageRole.FAST)
    except Exception:
        return None


def _format_knowledge_results(results: List[Dict]) -> str:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢"""
    if not results:
        return "ï¼ˆé–¢é€£ã™ã‚‹å†…éƒ¨çŸ¥è­˜ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"

    parts = []
    for i, item in enumerate(results, 1):
        title = item.get("title", "ç„¡é¡Œ")
        summary = item.get("summary", "")
        topics = ", ".join(item.get("topics", []))
        score = item.get("score", 0)
        parts.append(
            f"[{i}] {title} (é–¢é€£åº¦: {score:.2f})\n"
            f"    è¦ç´„: {summary}\n"
            f"    ãƒˆãƒ”ãƒƒã‚¯: {topics}"
        )
    return "\n".join(parts)


async def _search_internal_knowledge(query: str) -> List[Dict]:
    """Layer 3 ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§å†…éƒ¨çŸ¥è­˜ã‚’æ¤œç´¢"""
    try:
        from app.services.layer3.knowledge_store import knowledge_store
        await knowledge_store.initialize()
        results = await knowledge_store.search_similar(
            query=query,
            limit=5,
            score_threshold=0.5,
        )
        return results
    except Exception as e:
        logger.warning("Internal knowledge search failed", metadata={"error": str(e)})
        return []


async def _check_sufficiency(
    provider: LLMProvider,
    query: str,
    knowledge_context: str,
    answer: str,
) -> Dict[str, Any]:
    """å†…éƒ¨çŸ¥è­˜ã®å›ç­”ãŒååˆ†ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    try:
        prompt = _SUFFICIENCY_CHECK_PROMPT.format(
            query=query,
            knowledge_context=knowledge_context,
            answer=answer,
        )
        result = await provider.generate_json(
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å›ç­”ã®å……è¶³åº¦ã‚’åˆ¤å®šã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.1,
        )
        return {
            "is_sufficient": bool(result.get("is_sufficient", True)),
            "reason": result.get("reason", ""),
            "suggested_research_query": result.get("suggested_research_query"),
        }
    except Exception as e:
        logger.warning("Sufficiency check failed", metadata={"error": str(e)})
        return {"is_sufficient": True, "reason": "", "suggested_research_query": None}


async def run_knowledge_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    çŸ¥è­˜ãƒãƒ¼ãƒ‰: å†…éƒ¨çŸ¥è­˜æ¤œç´¢ + å›ç­”ç”Ÿæˆ + è¿½åŠ èª¿æŸ»ææ¡ˆ

    1. Layer 3 ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§å†…éƒ¨çŸ¥è­˜ã‚’å–å¾—
    2. LLM ã§å†…éƒ¨çŸ¥è­˜ã‚’è¸ã¾ãˆãŸå›ç­”ã‚’ç”Ÿæˆ
    3. requires_deep_research ãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ã„ã‚‹å ´åˆã€å›ç­”ã®å……è¶³åº¦ã‚’åˆ¤å®š
    4. ä¸ååˆ†ãªå ´åˆã¯è¿½åŠ èª¿æŸ»ã®ææ¡ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä»˜åŠ ã—ã€çŠ¶æ…‹ã«è¨˜éŒ²
    """
    input_text = state["input_text"]
    user_id = state.get("user_id", "")
    requires_deep_research = state.get("requires_deep_research", False)
    provider = _get_provider()

    if not provider:
        return {
            "response": "ãŠèª¿ã¹ã—ã¾ã™ã€‚å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚",
            "background_task_info": None,
        }

    try:
        await provider.initialize()

        # A. Layer 3 å†…éƒ¨çŸ¥è­˜æ¤œç´¢
        logger.info("Searching internal knowledge base", metadata={"query_preview": input_text[:100]})
        knowledge_results = await _search_internal_knowledge(input_text)
        knowledge_context = _format_knowledge_results(knowledge_results)
        logger.info(
            "Internal knowledge search completed",
            metadata={"result_count": len(knowledge_results)},
        )

        # B. å†…éƒ¨çŸ¥è­˜ã‚’è¸ã¾ãˆãŸå³æ™‚å›ç­”ã®ç”Ÿæˆ
        system_prompt = _KNOWLEDGE_ANSWER_PROMPT.format(
            knowledge_context=knowledge_context,
        )
        logger.info("LLM request (knowledge answer)", metadata={"prompt_preview": input_text[:100]})
        answer_result = await provider.generate_text(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": input_text},
            ],
            temperature=0.3,
        )
        quick_answer = answer_result.content
        logger.info("LLM response (knowledge answer)", metadata={"response_preview": quick_answer[:100]})

        # C. è¿½åŠ èª¿æŸ»ã®ææ¡ˆåˆ¤å®šï¼ˆrequires_deep_research ãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ã„ã‚‹å ´åˆï¼‰
        research_offered = False
        pending_research_query = None

        if requires_deep_research:
            sufficiency = await _check_sufficiency(
                provider, input_text, knowledge_context, quick_answer,
            )
            logger.info(
                "Sufficiency check result",
                metadata={
                    "is_sufficient": sufficiency["is_sufficient"],
                    "reason": sufficiency["reason"],
                },
            )

            if not sufficiency["is_sufficient"]:
                # è¿½åŠ èª¿æŸ»ã®ææ¡ˆã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ«å°¾ã«ä»˜åŠ 
                research_query = sufficiency.get("suggested_research_query") or input_text
                proposal_message = (
                    "\n\n---\n"
                    "ğŸ’¡ **ä»Šã®ç§ã®çŸ¥è­˜ï¼ˆDBï¼‰ã§ã¯ã“ã“ã¾ã§åˆ†ã‹ã£ã¦ã„ã¾ã™ã€‚**"
                    "ã•ã‚‰ã«æœ€æ–°ã®æƒ…å ±ã‚’å¤–éƒ¨ã‹ã‚‰è©³ã—ãèª¿ã¹ã¾ã—ã‚‡ã†ã‹ï¼Ÿ\n"
                    "ã€ŒãŠé¡˜ã„ã€ã¨è¨€ã£ã¦ã„ãŸã ã‘ã‚Œã°ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ»ãƒªã‚µãƒ¼ãƒã‚’é–‹å§‹ã—ã¾ã™ã€‚"
                )
                quick_answer += proposal_message
                research_offered = True
                pending_research_query = research_query
                logger.info(
                    "Research proposal offered",
                    metadata={"research_query": research_query[:100]},
                )

        return {
            "response": quick_answer,
            "background_task_info": None,
            "research_offered": research_offered,
            "pending_research_query": pending_research_query,
        }

    except Exception as e:
        logger.warning("LLM call failed", metadata={"error": str(e)})
        return {
            "response": "ãŠèª¿ã¹ã—ã¾ã™ã€‚å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚",
            "background_task_info": None,
        }
