"""
MINDYARD - Celery Tasks
Layer 2 ã®éåŒæœŸå‡¦ç†ã‚¿ã‚¹ã‚¯
"""
import asyncio
import logging
from typing import Optional
import uuid
import re

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm.attributes import flag_modified

from app.workers.celery_app import celery_app
from app.db.base import async_session_maker, engine
from app.models.raw_log import RawLog, LogIntent
from app.models.insight import InsightCard, InsightStatus
from app.services.layer1.context_analyzer import context_analyzer
from app.services.layer2.privacy_sanitizer import privacy_sanitizer
from app.services.layer2.insight_distiller import insight_distiller
from app.services.layer2.sharing_broker import sharing_broker
from app.services.layer2.structural_analyzer import structural_analyzer, is_continuation_phrase
from app.core.config import settings

logger = logging.getLogger(__name__)


def run_async(coro):
    """éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ"""
    loop = asyncio.get_event_loop()
    if loop.is_running():
        # æ—¢å­˜ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒã‚ã‚‹å ´åˆã¯æ–°ã—ã„ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, coro)
            return future.result()
    return loop.run_until_complete(coro)


@celery_app.task(bind=True, max_retries=3)
def analyze_log_context(self, log_id: str):
    """
    Layer 1: Context Analyzer ã‚¿ã‚¹ã‚¯
    ãƒ­ã‚°ã®æ„Ÿæƒ…ãƒ»ãƒˆãƒ”ãƒƒã‚¯ãƒ»ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚’è§£æ
    """
    async def _analyze():
        # Ensure engine connection pool is clean for this process/task
        # This prevents issues with inherited connections in forked processes
        await engine.dispose()

        async with async_session_maker() as session:
            # ãƒ­ã‚°ã‚’å–å¾—
            result = await session.execute(
                select(RawLog).where(RawLog.id == uuid.UUID(log_id))
            )
            log = result.scalar_one_or_none()

            if not log:
                logger.error(f"Log not found for context analysis: {log_id}")
                return {"status": "error", "message": "Log not found"}

            if log.is_analyzed:
                logger.info(f"Log already analyzed for context: {log_id}")
                return {"status": "skipped", "message": "Already analyzed"}

            try:
                logger.info(f"Starting context analysis for log_id: {log_id}")
                # è§£æå®Ÿè¡Œ
                analysis = await context_analyzer.analyze(log.content)

                # çµæœã‚’ä¿å­˜
                log.intent = analysis.get("intent")
                log.emotions = analysis.get("emotions")
                log.emotion_scores = analysis.get("emotion_scores")
                log.topics = analysis.get("topics")
                log.tags = analysis.get("tags")
                log.metadata_analysis = analysis.get("metadata_analysis")
                log.is_analyzed = True

                # Mark JSON/Array fields as modified to ensure SQLAlchemy detects changes
                flag_modified(log, "emotion_scores")
                flag_modified(log, "metadata_analysis")

                logger.info(f"Committing context analysis for log_id: {log_id}")
                await session.commit()
                logger.info(f"Successfully committed context analysis for log_id: {log_id}")

                return {
                    "status": "success",
                    "log_id": log_id,
                    "intent": str(log.intent) if log.intent else None,
                    "emotions": log.emotions,
                    "tags": log.tags,
                }

            except Exception as e:
                logger.error(f"Error in analyze_log_context for {log_id}: {str(e)}", exc_info=True)
                return {"status": "error", "message": str(e)}

    return run_async(_analyze())


@celery_app.task(bind=True, max_retries=3)
def analyze_log_structure(self, log_id: str):
    """
    Layer 2: Structural Analyzer ã‚¿ã‚¹ã‚¯
    æ–‡è„ˆä¾å­˜å‹ãƒ»æ§‹é€ çš„ç†è§£ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ

    éå»ã®ä¼šè©±å±¥æ­´ã¨ç›´å‰ã®æ§‹é€ çš„ç†è§£ã‚’è¸ã¾ãˆã€
    æ–°ã—ã„ãƒ­ã‚°ã®é–¢ä¿‚æ€§ã‚’åˆ¤å®šã—æ§‹é€ çš„èª²é¡Œã‚’æ›´æ–°ã™ã‚‹ã€‚
    """
    async def _analyze_structure():
        # Ensure engine connection pool is clean for this process/task
        await engine.dispose()

        async with async_session_maker() as session:
            # ç¾åœ¨ã®ãƒ­ã‚°ã‚’å–å¾—
            result = await session.execute(
                select(RawLog).where(RawLog.id == uuid.UUID(log_id))
            )
            log = result.scalar_one_or_none()

            if not log:
                logger.error(f"Log not found for structural analysis: {log_id}")
                return {"status": "error", "message": "Log not found"}

            if log.is_structure_analyzed:
                logger.info(f"Log already analyzed for structure: {log_id}")
                return {"status": "skipped", "message": "Already analyzed for structure"}

            # çŠ¶æ…‹ãƒ­ã‚°ã¯æ§‹é€ åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¿”ã™
            if log.intent == LogIntent.STATE or log.intent == "state":
                logger.info(f"Generating micro-feedback for state log: {log_id}")
                try:
                    analysis = await structural_analyzer.generate_state_feedback(
                        content=log.content,
                        emotions=log.emotions,
                    )
                    log.structural_analysis = analysis
                    log.is_structure_analyzed = True
                    flag_modified(log, "structural_analysis")
                    await session.commit()
                    logger.info(f"State micro-feedback saved for log_id: {log_id}")
                    return {
                        "status": "success",
                        "log_id": log_id,
                        "relationship_type": analysis.get("relationship_type"),
                        "probing_question": analysis.get("probing_question"),
                    }
                except Exception as e:
                    logger.error(f"Error generating state feedback for {log_id}: {str(e)}", exc_info=True)
                    return {"status": "error", "message": str(e)}

            try:
                logger.info(f"Starting structural analysis for log_id: {log_id}")
                # --- Topic overlap filter to avoid irrelevant history ---
                def _normalize(text: str):
                    tokens = re.split(r"[^\\wã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]+", text.lower())
                    stop = {"ã®", "ã«", "ã¨", "ã§", "ãŒ", "ã¯", "ã‚’", "ã‚‚", "ã¸", "and", "or", "the", "a", "an"}
                    return [t for t in tokens if len(t) > 1 and t not in stop]

                current_tokens = set(_normalize(log.content))
                # ã€Œç¶šãã‹ã‚‰ã€ã€Œç¶šãã§ã€ç­‰ã®ã¨ãã¯ãƒˆãƒ¼ã‚¯ãƒ³é‡ãªã‚Šã§å±¥æ­´ã‚’æ¨ã¦ãªã„ï¼ˆç›´å‰ãƒ­ã‚°ã‚’å¿…ãšä½¿ã†ï¼‰
                use_overlap_filter = not is_continuation_phrase(log.content or "")

                # Step 1: å±¥æ­´å–å¾— - åŒä¸€ã‚¹ãƒ¬ãƒƒãƒ‰ãŒã‚ã‚Œã°ãã®ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã®ç›´è¿‘5ä»¶ã€ãªã‘ã‚Œã°å¾“æ¥ã©ãŠã‚Šãƒ¦ãƒ¼ã‚¶ãƒ¼ç›´è¿‘5ä»¶
                history_query = select(RawLog).where(
                    RawLog.user_id == log.user_id,
                    RawLog.id != log.id,
                ).order_by(RawLog.created_at.desc()).limit(5)
                if getattr(log, "thread_id", None) is not None:
                    history_query = history_query.where(RawLog.thread_id == log.thread_id)
                history_result = await session.execute(history_query)
                candidates = history_result.scalars().all()

                past_logs = []
                for prev in candidates:
                    if use_overlap_filter:
                        overlap = current_tokens & set(_normalize(prev.content))
                        if overlap:
                            past_logs.append(prev)
                        else:
                            logger.info(f"[structural] skip history log {prev.id} (no topical overlap)")
                    else:
                        past_logs.append(prev)

                # Step 2: å‰å›ä»®èª¬ã®æŠ½å‡º
                previous_hypothesis = None
                if past_logs:
                    latest_prev_log = past_logs[0]
                    if latest_prev_log.structural_analysis:
                        # updated_structural_issue ã‚’å„ªå…ˆã€ãªã‘ã‚Œã° structural_issue
                        previous_hypothesis = (
                            latest_prev_log.structural_analysis.get("updated_structural_issue")
                            or latest_prev_log.structural_analysis.get("structural_issue")
                        )

                # Step 3: è¦ç´„ãƒªã‚¹ãƒˆä½œæˆ
                recent_history = []
                for prev_log in past_logs:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çŸ­ãåˆ‡ã‚Šè©°ã‚ã‚‹ï¼ˆ100æ–‡å­—ã¾ã§ï¼‰
                    summary = prev_log.content[:100]
                    if len(prev_log.content) > 100:
                        summary += "..."
                    recent_history.append(summary)

                # Step 4: æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®æœ€å¤§å€¤ã‚’å–å¾—
                max_emotion_score = 0.0
                if log.emotion_scores and isinstance(log.emotion_scores, dict):
                    scores = log.emotion_scores.values()
                    max_emotion_score = max(scores) if scores else 0.0

                # Step 5: StructuralAnalyzer å®Ÿè¡Œ
                analysis = await structural_analyzer.analyze(
                    current_log=log.content,
                    recent_history=recent_history if recent_history else None,
                    previous_hypothesis=previous_hypothesis,
                    max_emotion_score=max_emotion_score,
                )

                # çµæœã‚’ä¿å­˜
                log.structural_analysis = analysis
                # Explicitly set the completion flag
                log.is_structure_analyzed = True

                # Mark JSON fields as modified to ensure SQLAlchemy detects changes
                flag_modified(log, "structural_analysis")

                logger.info(f"Committing structural analysis for log_id: {log_id}")
                await session.commit()
                logger.info(f"Successfully committed structural analysis for log_id: {log_id}")

                return {
                    "status": "success",
                    "log_id": log_id,
                    "relationship_type": analysis.get("relationship_type"),
                    "updated_structural_issue": analysis.get("updated_structural_issue"),
                    "probing_question": analysis.get("probing_question"),
                }

            except Exception as e:
                logger.error(f"Error in analyze_log_structure for {log_id}: {str(e)}", exc_info=True)
                return {"status": "error", "message": str(e)}

    return run_async(_analyze_structure())


@celery_app.task(bind=True, max_retries=3)
def process_log_for_insight(self, log_id: str):
    """
    Layer 2: Gateway Refinery ã‚¿ã‚¹ã‚¯
    ãƒ­ã‚°ã‚’åŒ¿ååŒ– â†’ æ§‹é€ åŒ– â†’ è©•ä¾¡ â†’ ä¿å­˜
    """
    async def _process():
        # Ensure engine connection pool is clean for this process/task
        await engine.dispose()

        async with async_session_maker() as session:
            # ãƒ­ã‚°ã‚’å–å¾—
            result = await session.execute(
                select(RawLog).where(RawLog.id == uuid.UUID(log_id))
            )
            log = result.scalar_one_or_none()

            if not log:
                logger.error(f"Log not found for insight processing: {log_id}")
                return {"status": "error", "message": "Log not found"}

            if log.is_processed_for_insight:
                logger.info(f"Log already processed for insight: {log_id}")
                return {"status": "skipped", "message": "Already processed"}

            try:
                logger.info(f"Starting insight processing for log_id: {log_id}")
                # Step 1: Privacy Sanitizer - åŒ¿ååŒ–
                sanitized_content, sanitize_metadata = await privacy_sanitizer.sanitize(
                    log.content
                )

                # Step 2: Insight Distiller - æ§‹é€ åŒ–ãƒ»æŠ½è±¡åŒ–
                distilled = await insight_distiller.distill(
                    sanitized_content,
                    metadata={
                        "intent": str(log.intent) if log.intent else None,
                        "emotions": log.emotions,
                        "topics": log.topics,
                        "tags": log.tags,
                    }
                )

                # Step 3: Sharing Broker - è©•ä¾¡
                evaluation = await sharing_broker.evaluate_sharing_value(distilled)

                # Step 4: InsightCard ã‚’ä½œæˆ
                insight = InsightCard(
                    author_id=log.user_id,
                    source_log_id=log.id,
                    title=distilled.get("title", ""),
                    context=distilled.get("context"),
                    problem=distilled.get("problem"),
                    solution=distilled.get("solution"),
                    summary=distilled.get("summary", ""),
                    topics=distilled.get("topics"),
                    tags=distilled.get("tags"),
                    sharing_value_score=evaluation.get("sharing_value_score", 0),
                    novelty_score=evaluation.get("novelty_score", 0),
                    generality_score=evaluation.get("generality_score", 0),
                    status=(
                        InsightStatus.PENDING_APPROVAL
                        if evaluation.get("should_propose", False)
                        else InsightStatus.DRAFT
                    ),
                )

                session.add(insight)
                log.is_processed_for_insight = True

                logger.info(f"Committing insight processing for log_id: {log_id}")
                await session.commit()
                logger.info(f"Successfully committed insight processing for log_id: {log_id}")

                await session.refresh(insight)

                return {
                    "status": "success",
                    "log_id": log_id,
                    "insight_id": str(insight.id),
                    "should_propose": evaluation.get("should_propose", False),
                    "sharing_value_score": evaluation.get("sharing_value_score", 0),
                }

            except Exception as e:
                logger.error(f"Error in process_log_for_insight for {log_id}: {str(e)}", exc_info=True)
                return {"status": "error", "message": str(e)}

    return run_async(_process())


@celery_app.task(bind=True, max_retries=3)
def deep_research_task(self, query: str, user_id: str):
    """
    Knowledge Node ã‹ã‚‰ã‚­ãƒƒã‚¯ã•ã‚Œã‚‹éåŒæœŸèª¿æŸ»ã‚¿ã‚¹ã‚¯

    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦DEEPãƒ¢ãƒ‡ãƒ«ã§è©³ç´°ãªèª¿æŸ»ã‚’è¡Œã„ã€
    çµæœã‚’Celeryãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰çµŒç”±ã§è¿”ã™ã€‚
    å°†æ¥çš„ã«WebSocketé€šçŸ¥ã‚„DBä¿å­˜ã«ã‚‚å¯¾å¿œå¯èƒ½ã€‚
    """
    async def _research():
        await engine.dispose()

        try:
            logger.info(
                f"Starting deep research for user_id: {user_id}, query: {query[:100]}"
            )

            from app.core.llm import llm_manager
            from app.core.llm_provider import LLMUsageRole

            provider = llm_manager.get_client(LLMUsageRole.DEEP)
            await provider.initialize()

            system_prompt = """ã‚ãªãŸã¯è©³ç´°ãªèª¿æŸ»ãƒ»ãƒªã‚µãƒ¼ãƒã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦ã€æ·±ãæ˜ã‚Šä¸‹ã’ãŸåŒ…æ‹¬çš„ãªèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
1. æ¦‚è¦: è³ªå•ã¸ã®ç·åˆçš„ãªå›ç­”
2. è©³ç´°åˆ†æ: å„è«–ç‚¹ã®æ˜ã‚Šä¸‹ã’
3. ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹: æ ¹æ‹ ã¨ãªã‚‹æƒ…å ±ãƒ»ãƒ‡ãƒ¼ã‚¿
4. çµè«–ã¨æ¨å¥¨: ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

æ—¥æœ¬èªã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚"""

            result = await provider.generate_text(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query},
                ],
                temperature=0.3,
            )

            logger.info(f"Deep research completed for user_id: {user_id}")

            return {
                "status": "success",
                "user_id": user_id,
                "query": query,
                "report": result.content,
            }

        except Exception as e:
            logger.error(
                f"Error in deep_research_task for user_id {user_id}: {str(e)}",
                exc_info=True,
            )
            return {"status": "error", "message": str(e)}

    return run_async(_research())


@celery_app.task(bind=True, max_retries=3)
def run_deep_research_task(
    self,
    user_id: str,
    thread_id: str,
    query: str,
    initial_context: str,
    research_log_id: str,
):
    """
    Deep Research éåŒæœŸã‚¿ã‚¹ã‚¯

    deep_research_node ã‹ã‚‰ã‚­ãƒƒã‚¯ã•ã‚Œã€DEEP ãƒ¢ãƒ‡ãƒ«ã§è©³ç´°èª¿æŸ»ã‚’è¡Œã„ã€
    çµæœã‚’ RawLog ã® assistant_reply ã«ä¿å­˜ã™ã‚‹ã€‚

    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        thread_id: ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰ID
        query: èª¿æŸ»ã‚¯ã‚¨ãƒªï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•ï¼‰
        initial_context: åˆå›å›ç­”ï¼ˆæ·±æ˜ã‚Šç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰
        research_log_id: çµæœã‚’ä¿å­˜ã™ã‚‹ RawLog ã® IDï¼ˆäº‹å‰ç”Ÿæˆï¼‰
    """
    async def _research():
        await engine.dispose()

        try:
            logger.info(
                f"Starting deep research task for user_id: {user_id}, "
                f"log_id: {research_log_id}, query: {query[:100]}"
            )

            from app.core.llm import llm_manager
            from app.core.llm_provider import LLMUsageRole

            provider = llm_manager.get_client(LLMUsageRole.DEEP)
            await provider.initialize()

            system_prompt = (
                "ã‚ãªãŸã¯MINDYARDã® Deep Research ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã€å¾¹åº•çš„ã‹ã¤åŒ…æ‹¬çš„ãªèª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
                "### å‡ºåŠ›åˆ¶ç´„ï¼ˆå³å®ˆï¼‰:\n"
                "- **æ–‡å­—æ•°: 2,000ã€œ3,000æ–‡å­—**ã«åã‚ã‚‹ã“ã¨ã€‚è¶…éç¦æ­¢ã€‚\n"
                "- è©³ç´°ã¯**ç®‡æ¡æ›¸ãï¼ˆãƒ» ã‚„ - ï¼‰**ã§ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹ã€‚\n"
                "- Markdownãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ã†å ´åˆã€**å„ã‚»ãƒ«ã¯50æ–‡å­—ä»¥å†…**ã«ã™ã‚‹ã“ã¨ã€‚\n"
                "- å†—é•·ãªå‰ç½®ããƒ»ç¹°ã‚Šè¿”ã—ã‚’é¿ã‘ã€æƒ…å ±å¯†åº¦ã‚’é«˜ãä¿ã¤ã€‚\n\n"
                "### èª¿æŸ»æ–¹é‡:\n"
                "1. **å¤šè§’çš„ãªè¦–ç‚¹**: è¤‡æ•°ã®è¦³ç‚¹ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æã™ã‚‹\n"
                "2. **æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”**: è¦‹å‡ºã—ãƒ»ç®‡æ¡æ›¸ãã‚’ä½¿ã£ã¦æƒ…å ±ã‚’æ•´ç†ã™ã‚‹\n"
                "3. **ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹**: ä¸»å¼µã«ã¯æ ¹æ‹ ã‚„å‡ºå…¸ã®æ–¹å‘æ€§ã‚’ç¤ºã™\n"
                "4. **å®Ÿç”¨æ€§é‡è¦–**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–ã‚Œã‚‹å…·ä½“çš„æƒ…å ±\n\n"
                "### å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:\n"
                "- æ¦‚è¦ï¼ˆ1-2æ–‡ã®ã‚µãƒãƒªãƒ¼ï¼‰\n"
                "- ä¸»è¦ãªç™ºè¦‹ãƒ»çŸ¥è¦‹ï¼ˆç®‡æ¡æ›¸ãï¼‰\n"
                "- è©³ç´°åˆ†æï¼ˆå„ãƒã‚¤ãƒ³ãƒˆã®æ˜ã‚Šä¸‹ã’ï¼‰\n"
                "- æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ææ¡ˆ\n\n"
                "### æ³¨æ„äº‹é …:\n"
                "- æ—¥æœ¬èªã§å¿œç­”ã™ã‚‹\n"
                "- ç¢ºè¨¼ã®ãªã„æƒ…å ±ã¯ã€Œã€œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€ç­‰ã¨æ˜è¨˜ã™ã‚‹\n"
                "- å°‚é–€ç”¨èªã«ã¯ç°¡æ½”ãªèª¬æ˜ã‚’ä»˜ã‘ã‚‹\n"
            )

            research_query = query
            if initial_context:
                research_query = (
                    f"å…ƒã®è³ªå•: {query}\n\n"
                    f"åˆå›ã®å›ç­”ï¼ˆã“ã‚Œã‚’æ·±æ˜ã‚Šã—ã¦ãã ã•ã„ï¼‰:\n{initial_context}"
                )

            result = await provider.generate_text(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": research_query},
                ],
                temperature=0.3,
            )

            research_report = result.content
            logger.info(
                f"Deep research completed for log_id: {research_log_id}, "
                f"length: {len(research_report)}"
            )

            # çµæœã‚’ RawLog ã«ä¿å­˜
            async with async_session_maker() as session:
                log = RawLog(
                    id=uuid.UUID(research_log_id),
                    user_id=uuid.UUID(user_id),
                    thread_id=uuid.UUID(thread_id) if thread_id else None,
                    content=query,
                    content_type="deep_research",
                    assistant_reply=f"ğŸ”¬ **Deep Research çµæœ**\n\n{research_report}",
                    is_analyzed=True,
                    is_structure_analyzed=True,
                    is_processed_for_insight=False,
                )
                session.add(log)
                await session.commit()
                logger.info(f"Deep research result saved to log_id: {research_log_id}")

            return {
                "status": "success",
                "log_id": research_log_id,
                "user_id": user_id,
                "report_length": len(research_report),
            }

        except Exception as e:
            logger.error(
                f"Error in run_deep_research_task for log_id {research_log_id}: {str(e)}",
                exc_info=True,
            )
            # ã‚¨ãƒ©ãƒ¼ã§ã‚‚çµæœã‚’ä¿å­˜ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥ã™ã‚‹ãŸã‚ï¼‰
            try:
                async with async_session_maker() as session:
                    log = RawLog(
                        id=uuid.UUID(research_log_id),
                        user_id=uuid.UUID(user_id),
                        thread_id=uuid.UUID(thread_id) if thread_id else None,
                        content=query,
                        content_type="deep_research",
                        assistant_reply=(
                            "Deep Research ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n"
                            "å†åº¦ãŠè©¦ã—ã„ãŸã ãã“ã¨ã‚‚ã§ãã¾ã™ã€‚"
                        ),
                        is_analyzed=True,
                        is_structure_analyzed=True,
                    )
                    session.add(log)
                    await session.commit()
            except Exception:
                logger.error(
                    f"Failed to save error log for {research_log_id}",
                    exc_info=True,
                )
            return {"status": "error", "message": str(e)}

    return run_async(_research())


@celery_app.task
def process_all_unprocessed_logs():
    """
    æœªå‡¦ç†ã®ãƒ­ã‚°ã‚’ã™ã¹ã¦å‡¦ç†ã™ã‚‹ãƒãƒƒãƒã‚¿ã‚¹ã‚¯
    """
    async def _process_all():
        # Ensure engine connection pool is clean for this process/task
        await engine.dispose()

        async with async_session_maker() as session:
            # æœªå‡¦ç†ã®ãƒ­ã‚°ã‚’å–å¾—
            result = await session.execute(
                select(RawLog).where(
                    RawLog.is_analyzed == True,
                    RawLog.is_processed_for_insight == False,
                ).limit(100)  # ãƒãƒƒãƒã‚µã‚¤ã‚º
            )
            logs = result.scalars().all()

            processed = []
            for log in logs:
                # å€‹åˆ¥ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                process_log_for_insight.delay(str(log.id))
                processed.append(str(log.id))

            return {
                "status": "success",
                "queued_count": len(processed),
                "log_ids": processed,
            }

    return run_async(_process_all())
